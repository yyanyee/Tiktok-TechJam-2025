{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "126b15ea"
      },
      "source": [
        "# Cell 1 — Installs & Imports\n",
        "\n",
        "This cell installs the necessary Python libraries for this notebook and imports all required modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf51bffd"
      },
      "source": [
        "import sys, subprocess\n",
        "\n",
        "def _pip(pkg):\n",
        "    \"\"\"Installs packages quietly using pip.\"\"\"\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + (pkg if isinstance(pkg, list) else [pkg]))\n",
        "\n",
        "# Core libraries\n",
        "_pip([\"pandas\", \"numpy\", \"scikit-learn\", \"scipy\", \"tqdm\", \"pyarrow\", \"fastparquet\", \"joblib\", \"unidecode\", \"orjson\"])\n",
        "# Hugging Face libraries for model handling\n",
        "_pip([\"transformers>=4.50.0\", \"accelerate\", \"huggingface_hub\", \"bitsandbytes\"])\n",
        "# Library for multilabel stratification\n",
        "_pip(\"iterative-stratification\")\n",
        "\n",
        "import os, re, gc, json, time, math, glob, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from unidecode import unidecode\n",
        "from huggingface_hub import snapshot_download, notebook_login\n",
        "from transformers import AutoProcessor, Gemma3ForConditionalGeneration, BitsAndBytesConfig\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support, precision_recall_curve\n",
        "from scipy.sparse import hstack\n",
        "import joblib\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d16952e"
      },
      "source": [
        "# Cell 2 — Config, Mount Drive, Paths, Seeds\n",
        "\n",
        "This cell sets up configuration parameters, mounts Google Drive, defines project paths, and sets random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a11e750d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "def set_seeds(seed=42):\n",
        "    \"\"\"Sets random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "set_seeds(42)\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "# Define project directories in Google Drive\n",
        "OUT_DIR   = \"/content/drive/MyDrive/Tiktok_Hackathon\"\n",
        "CACHE_DIR = f\"{OUT_DIR}/hf_cache\"     # persistent HF cache\n",
        "CKPT_DIR  = f\"{OUT_DIR}/labels_ckpt_1k\" # checkpoint directory for labeling\n",
        "for d in [OUT_DIR, CACHE_DIR, CKPT_DIR]: os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Route Hugging Face caches to Drive for persistence\n",
        "os.environ[\"HF_HOME\"] = CACHE_DIR\n",
        "os.environ[\"HF_HUB_CACHE\"] = CACHE_DIR\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = CACHE_DIR\n",
        "os.environ.setdefault(\"HF_HUB_ENABLE_HF_TRANSFER\", \"1\") # Enable faster downloads\n",
        "\n",
        "# Raw data paths - UPDATE these paths if your files are different\n",
        "REVIEW_PATH = f\"{OUT_DIR}/review-Vermont.json\"\n",
        "META_PATH   = f\"{OUT_DIR}/meta-Vermont.json\"\n",
        "\n",
        "# Configuration for labeling and preview\n",
        "N_PREVIEW = 30       # number of samples for quick preview\n",
        "N_LABEL   = 1000     # target number of samples for checkpointed labeling\n",
        "PART_SIZE = 100      # number of rows per checkpoint file\n",
        "\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n",
        "print(\"CACHE_DIR:\", CACHE_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2fcf37e"
      },
      "source": [
        "# Cell 3 — Load & prep raw data → df\n",
        "\n",
        "This cell reads the raw JSONL review and metadata files, merges them based on `gmap_id`, removes duplicate entries, and renames key columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88019663"
      },
      "source": [
        "import json as _json\n",
        "\n",
        "def parse_jsonl(path, limit=None):\n",
        "    \"\"\"Parses a JSONL file and returns a list of dictionaries.\"\"\"\n",
        "    data = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if limit and i >= limit: break\n",
        "            data.append(_json.loads(line))\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Assert that the input files exist\n",
        "assert os.path.exists(REVIEW_PATH) and os.path.exists(META_PATH), \"Review/meta files not found in OUT_DIR.\"\n",
        "\n",
        "# Load data into DataFrames\n",
        "df_reviews = parse_jsonl(REVIEW_PATH)\n",
        "df_meta    = parse_jsonl(META_PATH)\n",
        "\n",
        "# Deduplicate businesses in metadata based on 'gmap_id'\n",
        "if 'gmap_id' in df_meta.columns:\n",
        "    df_meta = df_meta.drop_duplicates(subset=['gmap_id'])\n",
        "\n",
        "# Merge reviews and metadata\n",
        "df = df_reviews.merge(df_meta, on='gmap_id', how='left')\n",
        "\n",
        "# Drop exact duplicate review rows\n",
        "keep_cols = [c for c in ['user_id','gmap_id','text','time','rating'] if c in df.columns]\n",
        "if keep_cols:\n",
        "    before = len(df)\n",
        "    df = df.drop_duplicates(subset=keep_cols, keep='first')\n",
        "    print(f\"Dropped {before - len(df)} duplicate review rows.\")\n",
        "\n",
        "# Rename columns for clarity if present\n",
        "ren = {}\n",
        "if 'name_x' in df.columns: ren['name_x'] = 'reviewer_name'\n",
        "if 'name_y' in df.columns: ren['name_y'] = 'business_name'\n",
        "if ren:\n",
        "    df = df.rename(columns=ren)\n",
        "\n",
        "print(\"df shape:\", df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f1af94b"
      },
      "source": [
        "# Cell 4 — Hugging Face login (for gated model access)\n",
        "\n",
        "This cell provides a way to log in to the Hugging Face Hub, which might be necessary to access certain gated models like `google/gemma-3-12b-it`. You only need to run this once per session if the model is gated for your account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "540b23e7"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "try:\n",
        "    notebook_login()\n",
        "except Exception as e:\n",
        "    print(\"HF login skipped or already authenticated.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02b34001"
      },
      "source": [
        "# Cell 5 — Load Gemma‑3 12B from Drive cache (download if needed)\n",
        "\n",
        "This cell loads the Gemma-3 12B Instruction Tuned model. It first checks for a cached version in the specified Drive cache directory (`CACHE_DIR`). If not found, it downloads the model from the Hugging Face Hub to the cache, resuming if necessary. The model is loaded with 4-bit quantization if a GPU is available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0602efdb"
      },
      "source": [
        "# Robust Gemma-3 12B loader: copies full snapshot to Drive (no hardlinks), then loads from there.\n",
        "\n",
        "import os, gc, torch, shutil\n",
        "from huggingface_hub import snapshot_download\n",
        "from transformers import AutoProcessor, Gemma3ForConditionalGeneration, BitsAndBytesConfig\n",
        "\n",
        "REPO_ID = \"google/gemma-3-12b-it\"  # gated: be logged in!\n",
        "OUT_DIR = \"/content/drive/MyDrive/Tiktok_Hackathon\"\n",
        "DRIVE_SNAPSHOT = f\"{OUT_DIR}/gemma3_12b_snapshot\"   # final folder with real files (not symlinks)\n",
        "\n",
        "def _has_all_shards(folder, n=5):\n",
        "    # basic presence/size check; adjust if HF changes naming\n",
        "    ok = True\n",
        "    sizes = []\n",
        "    for i in range(1, n+1):\n",
        "        p = os.path.join(folder, f\"model-0000{i}-of-0000{n}.safetensors\")\n",
        "        ok &= os.path.exists(p)\n",
        "        sizes.append((p, os.path.getsize(p) if os.path.exists(p) else 0))\n",
        "    return ok, sizes\n",
        "\n",
        "# 1) Reuse snapshot if it looks complete; else (re)download & copy here as real files\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "reuse = False\n",
        "if os.path.isdir(DRIVE_SNAPSHOT):\n",
        "    have, sizes = _has_all_shards(DRIVE_SNAPSHOT, n=5)\n",
        "    reuse = have and all(sz > 1_000_000_000 for _, sz in sizes)  # each > ~1GB\n",
        "\n",
        "if not reuse:\n",
        "    # Download to the default local cache, then copy to Drive folder as plain files (no links)\n",
        "    print(\"⬇️  Downloading Gemma-3 12B to local cache… (ensure you're logged into HF)\")\n",
        "    local_snap = snapshot_download(\n",
        "        repo_id=REPO_ID,\n",
        "        allow_patterns=[\"*.json\",\"*.safetensors\",\"*.model\",\"*.tokenizer*\",\"*.txt\",\"*.md\",\"*config*\",\"chat_template.json\"],\n",
        "    )\n",
        "    print(\"📁 Local snapshot:\", local_snap)\n",
        "\n",
        "    # Clear stale/partial Drive snapshot (if any), then copy fresh\n",
        "    if os.path.isdir(DRIVE_SNAPSHOT):\n",
        "        shutil.rmtree(DRIVE_SNAPSHOT)\n",
        "    shutil.copytree(local_snap, DRIVE_SNAPSHOT, dirs_exist_ok=True)\n",
        "    have, sizes = _has_all_shards(DRIVE_SNAPSHOT, n=5)\n",
        "    assert have, f\"Drive snapshot incomplete after copy: {sizes}\"\n",
        "    print(\"✅ Copied full snapshot to:\", DRIVE_SNAPSHOT)\n",
        "\n",
        "# 2) Load from Drive snapshot (fast & reliable)\n",
        "quant_cfg = None\n",
        "dtype = torch.float32\n",
        "if torch.cuda.is_available():\n",
        "    quant_cfg = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    dtype = torch.bfloat16\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(DRIVE_SNAPSHOT, trust_remote_code=True, use_fast=True)\n",
        "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
        "    DRIVE_SNAPSHOT,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    quantization_config=quant_cfg,\n",
        "    torch_dtype=dtype,\n",
        "    trust_remote_code=True,\n",
        ").eval()\n",
        "\n",
        "print(\"🎉 Ready:\", REPO_ID, \"| device:\", next(model.parameters()).device, \"| dtype:\", model.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "577028eb"
      },
      "source": [
        "# Cell 6 — Prompt, parser, guard (LLM labeling)\n",
        "\n",
        "This cell defines the system rules and user template for prompting the Gemma model, along with functions to clean review text, check for low-quality reviews, build messages for the model, and parse the model's JSON output, including a guard function to handle low-quality inputs efficiently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddc64ccf"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import orjson as _oj # Using orjson for faster JSON handling\n",
        "from unidecode import unidecode\n",
        "import torch\n",
        "\n",
        "SYSTEM_RULES = (\"\"\"\n",
        "You are a strict moderator for Google location reviews.\n",
        "Decide ONLY from the review text plus the provided place name and category. Do not use outside knowledge.\n",
        "If a visit is not clearly implied, do not assume one. Multiple violations may apply.\n",
        "If uncertain, set all violations to false and set is_relevant to false.\n",
        "\n",
        "Definitions you must apply:\n",
        "- ADS (Advertisements or promos): URLs, phone numbers, coupon codes, discount percentages, phrases like call now, use code, any self or third party promotion.\n",
        "- IRRELEVANT (off topic): Mostly about unrelated topics or a different place or brand with no substantive content about this place.\n",
        "- NON_VISIT_RANT: Complaints without clear visit signals such as never been, I heard, did not go inside, will not go. Phone only complaint counts unless the business is inherently phone service.\n",
        "- SPAM or LOW_QUALITY: Near empty or one word or emoji only, gibberish, excessive repetition, boilerplate or copy paste.\n",
        "\n",
        "Relevancy:\n",
        "- true if the text substantively describes the experience, service, staff, products, or environment of the named place or clearly matches its category.\n",
        "- false if IRRELEVANT or pure SPAM. ADS can be present while still relevant.\n",
        "\n",
        "Return JSON only in this exact schema:\n",
        "{\n",
        "  \"is_relevant\": true|false,\n",
        "  \"violations\": [\"ADS\"|\"IRRELEVANT\"|\"NON_VISIT_RANT\"|\"SPAM\", ...],\n",
        "  \"rationale\": \"<=25 words\"\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "USER_TMPL = \"\"\"Place:\n",
        "  name: {place}\n",
        "  category: {cat}\n",
        "Review:\n",
        "  text: {text}\n",
        "\n",
        "Decide using the rules. Return JSON only in the schema above.\n",
        "\"\"\"\n",
        "\n",
        "GT_WRAPPER = re.compile(r'\\(Translated by Google\\)\\s*|\\(Original\\)\\s*', re.I)\n",
        "LABELS_SET = {\"ADS\",\"IRRELEVANT\",\"NON_VISIT_RANT\",\"SPAM\"}\n",
        "\n",
        "def _norm(s):\n",
        "    \"\"\"Normalizes strings by removing extra whitespace and handling unicode.\"\"\"\n",
        "    s = \"\" if s is None else str(s)\n",
        "    return re.sub(r\"\\s+\",\" \", unidecode(s).replace(\"\\u200b\",\" \").strip())\n",
        "\n",
        "def clean_review(s: str) -> str:\n",
        "    \"\"\"Cleans review text by removing translation wrappers and normalizing.\"\"\"\n",
        "    s = _norm(s)\n",
        "    s = GT_WRAPPER.sub('', s)\n",
        "    return s.strip()\n",
        "\n",
        "def cheap_low_quality(s: str) -> bool:\n",
        "    \"\"\"Quickly checks if a review is likely low quality (spam, too short, etc.).\"\"\"\n",
        "    s2 = clean_review(s or \"\")\n",
        "    if not s2: return True\n",
        "    wc = len(s2.split())\n",
        "    short = wc <= 2 or len(s2) < 12\n",
        "    emoji_only = bool(s2) and not re.search(r'[A-Za-z]', s2)\n",
        "    repeated = bool(re.search(r'(.)\\1{3,}', s2))\n",
        "    return short or emoji_only or repeated\n",
        "\n",
        "def build_messages(text, place=\"\", cat=\"\"):\n",
        "    \"\"\"Builds the conversation messages for the LLM prompt.\"\"\"\n",
        "    user = USER_TMPL.format(place=_norm(place), cat=_norm(cat), text=_norm(text)[:1000])\n",
        "    return [\n",
        "        {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_RULES}]},\n",
        "        {\"role\":\"user\",  \"content\":[{\"type\":\"text\",\"text\":user}]},\n",
        "    ]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def gemma_label_one(text, place=\"\", cat=\"\", max_new_tokens=96):\n",
        "    \"\"\"Labels a single review using the Gemma model.\"\"\"\n",
        "    # Apply chat template and tokenize\n",
        "    enc = processor.apply_chat_template(\n",
        "        build_messages(text, place, cat),\n",
        "        add_generation_prompt=True, tokenize=True,\n",
        "        return_tensors=\"pt\", return_dict=True\n",
        "    )\n",
        "    enc = {k: v.to(model.device) for k,v in enc.items()}\n",
        "\n",
        "    # Generate response from the model\n",
        "    out = model.generate(\n",
        "        **enc, max_new_tokens=max_new_tokens, do_sample=False,\n",
        "        pad_token_id=processor.tokenizer.eos_token_id,\n",
        "        eos_token_id=processor.tokenizer.eos_token_id,\n",
        "    )\n",
        "    gen = out[0][enc[\"input_ids\"].shape[-1]:]\n",
        "    txt = processor.tokenizer.decode(gen, skip_special_tokens=True)\n",
        "\n",
        "    # Extract JSON from the model's output\n",
        "    m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", txt, re.S|re.I) or re.search(r\"\\{.*\\}\", txt, re.S)\n",
        "    data = {}\n",
        "    if m:\n",
        "        raw = m.group(1) if m.re.pattern.startswith(\"```json\") else m.group(0)\n",
        "        try:\n",
        "            data = _oj.loads(raw) # Use orjson for loading\n",
        "        except Exception:\n",
        "            try:\n",
        "                data = json.loads(raw) # Fallback to standard json\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to parse JSON: {e} - Raw: {raw}\")\n",
        "\n",
        "\n",
        "    # Parse and validate the extracted data\n",
        "    is_rel = bool(data.get(\"is_relevant\", False))\n",
        "    vs = data.get(\"violations\", [])\n",
        "    if isinstance(vs, str): vs=[vs] # Handle case where violations is a single string\n",
        "    viols = sorted({v.upper().replace(\" \",\"_\") for v in vs if v})\n",
        "    viols = [v for v in viols if v in LABELS_SET] # Filter for defined labels\n",
        "\n",
        "    return {\n",
        "        \"is_relevant_overall\": int(is_rel),\n",
        "        \"is_advertisement\":    int(\"ADS\" in viols),\n",
        "        \"is_irrelevant\":       int(\"IRRELEVANT\" in viols),\n",
        "        \"is_rant_without_visit\": int(\"NON_VISIT_RANT\" in viols),\n",
        "        \"is_spam\":             int(\"SPAM\" in viols),\n",
        "        \"rationale\": str(data.get(\"rationale\",\"\"))[:200].strip(),\n",
        "        \"gemma_raw\": txt,\n",
        "        \"parse_success\": int(bool(data) and all(v in data for v in [\"is_relevant\", \"violations\", \"rationale\"])), # Check for minimal required keys\n",
        "    }\n",
        "\n",
        "def label_with_guard(text, place=\"\", cat=\"\"):\n",
        "    \"\"\"Labels a review using Gemma, with a cheap pre-check for low quality.\"\"\"\n",
        "    if cheap_low_quality(text):\n",
        "        # Return a quick label for low-quality reviews\n",
        "        return {\n",
        "            \"is_relevant_overall\": 0,\n",
        "            \"is_advertisement\": 0,\n",
        "            \"is_irrelevant\": 0,\n",
        "            \"is_rant_without_visit\": 0,\n",
        "            \"is_spam\": 1,\n",
        "            \"rationale\": \"Empty/near-empty or low-quality review.\",\n",
        "            \"gemma_raw\": \"\", # No Gemma call was made\n",
        "            \"parse_success\": 1, # Considered a successful \"parse\" for the guard\n",
        "        }\n",
        "    return gemma_label_one(clean_review(text), place, cat) # Label with Gemma if not low-quality"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda11189"
      },
      "source": [
        "# Cell 7 — Label a small preview (N_PREVIEW) & peek\n",
        "\n",
        "This cell labels a small, random sample of the reviews (`N_PREVIEW`) using the defined `label_with_guard` function and displays the first few results. This is useful for a quick sanity check of the labeling process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de0ea9f0"
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Ensure df and necessary variables are available\n",
        "assert 'df' in globals() and 'label_with_guard' in globals(), \"Make sure to run previous cells.\"\n",
        "N_PREVIEW = 5\n",
        "# Sample a small preview of the data\n",
        "preview = df.sample(n=min(N_PREVIEW, len(df)), random_state=42).copy()\n",
        "rows = []\n",
        "\n",
        "# Label each review in the preview\n",
        "for r in tqdm(preview.itertuples(index=False), total=len(preview), desc=\"preview label\"):\n",
        "    t   = getattr(r, 'text', '')\n",
        "    # Get business name and category if columns exist\n",
        "    plc = getattr(r, 'business_name', '') if 'business_name' in preview.columns else ''\n",
        "    cat = getattr(r, 'category', '') if 'category' in preview.columns else ''\n",
        "    rows.append(label_with_guard(t, plc, cat))\n",
        "\n",
        "# Concatenate the original preview data with the new labels\n",
        "preview = pd.concat([preview.reset_index(drop=True), pd.DataFrame(rows)], axis=1)\n",
        "\n",
        "# Display the first 5 labeled reviews\n",
        "display(preview.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ea24890"
      },
      "source": [
        "# Cell 8 — Label N_LABEL with checkpoints and consolidate\n",
        "\n",
        "This cell labels a larger sample of reviews (`N_LABEL`) with checkpointing. It saves the labeled results periodically to Google Drive (`CKPT_DIR`) to avoid losing progress. If checkpoint files are found, it attempts to consolidate them or resume labeling from where it left off. Finally, it consolidates all parts into a single Parquet and CSV file and prints basic statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70b48a9e"
      },
      "source": [
        "import os, glob, math, pandas as pd, torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Ensure necessary variables and directories are defined\n",
        "OUT_DIR = globals().get('OUT_DIR', '/content/drive/MyDrive/Tiktok_Hackathon') # Use defined OUT_DIR or default\n",
        "CKPT_DIR_1K = f\"{OUT_DIR}/labels_ckpt_1k\" # Checkpoint directory for 1k labels\n",
        "PREVIEW_PARQUET = f\"{OUT_DIR}/labeled_preview_1000.parquet\" # Final output Parquet file\n",
        "PREVIEW_CSV     = f\"{OUT_DIR}/labeled_preview_1000.csv\"     # Final output CSV file\n",
        "\n",
        "# Ensure Drive is mounted (redundant if Cell 2 ran, but safe check)\n",
        "if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Ensure checkpoint directory exists\n",
        "os.makedirs(CKPT_DIR_1K, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- Labeling Logic ---\n",
        "\n",
        "# 1) Check if the final labeled preview file already exists\n",
        "if os.path.exists(PREVIEW_PARQUET) or os.path.exists(PREVIEW_CSV):\n",
        "    print(\"✅ Found existing labeled preview — skipping labeling.\")\n",
        "    labeled_1k = (pd.read_parquet(PREVIEW_PARQUET)\n",
        "                  if os.path.exists(PREVIEW_PARQUET) # Prefer Parquet\n",
        "                  else pd.read_csv(PREVIEW_CSV))\n",
        "    print({\"rows\": len(labeled_1k)})\n",
        "else:\n",
        "    # 2) If checkpoint parts exist, consolidate them\n",
        "    part_files = sorted(glob.glob(f\"{CKPT_DIR_1K}/part_*.parquet\"))\n",
        "    if part_files:\n",
        "        print(f\"ℹ️ Found {len(part_files)} checkpoint parts — consolidating.\")\n",
        "        labeled_1k = pd.concat([pd.read_parquet(p) for p in part_files], ignore_index=True)\n",
        "        # Save consolidated parts as the final preview\n",
        "        labeled_1k.to_parquet(PREVIEW_PARQUET, index=False)\n",
        "        labeled_1k.to_csv(PREVIEW_CSV, index=False)\n",
        "        print(\"💾 Saved consolidated preview:\", PREVIEW_PARQUET, \"|\", PREVIEW_CSV)\n",
        "    else:\n",
        "        # 3) No preview or parts found, start fresh labeling with checkpoints\n",
        "        assert 'df' in globals() and 'text' in df.columns, \"Need df with a 'text' column. Run Cell 3.\"\n",
        "        assert 'label_with_guard' in globals(), \"Run the cell that defines label_with_guard() first (Cell 6).\"\n",
        "\n",
        "        N_TARGET = min(N_LABEL, len(df)) # Label up to N_LABEL or the total number of reviews if less\n",
        "        sample_1k = df.sample(n=N_TARGET, random_state=42).copy() # Sample data for labeling\n",
        "        sample_1k['orig_index'] = sample_1k.index # Keep original index for reference\n",
        "        sample_1k = sample_1k.reset_index(drop=True) # Reset index for chunking\n",
        "\n",
        "        # Optional manifest file to record the sampled data\n",
        "        manifest_path = f\"{CKPT_DIR_1K}/manifest.parquet\"\n",
        "        if not os.path.exists(manifest_path):\n",
        "            cols = ['orig_index','text'] + [c for c in ['business_name','category'] if c in sample_1k.columns]\n",
        "            sample_1k[cols].to_parquet(manifest_path, index=False)\n",
        "\n",
        "        CHUNK = PART_SIZE # Chunk size for checkpointing\n",
        "        parts = math.ceil(N_TARGET / CHUNK) # Calculate number of chunks\n",
        "        print(f\"Labeling {N_TARGET} rows → {parts} chunks x {CHUNK}\")\n",
        "\n",
        "        labeled_parts = [] # List to store labeled chunks\n",
        "        for p in range(parts):\n",
        "            part_path = f\"{CKPT_DIR_1K}/part_{p:04d}.parquet\"\n",
        "            if os.path.exists(part_path):\n",
        "                print(f\"⏩ Skipping chunk {p+1}/{parts} (already exists).\")\n",
        "                labeled_parts.append(pd.read_parquet(part_path))\n",
        "                continue  # Resume: skip if part file already exists\n",
        "\n",
        "            lo, hi = p*CHUNK, min((p+1)*CHUNK, N_TARGET) # Define chunk slice\n",
        "            sub = sample_1k.iloc[lo:hi].copy() # Get the current chunk\n",
        "\n",
        "            rows = []\n",
        "            # Label each review in the current chunk\n",
        "            for r in tqdm(sub.itertuples(index=False), total=len(sub), desc=f\"chunk {p+1}/{parts}\"):\n",
        "                t   = getattr(r, 'text', '')\n",
        "                plc = getattr(r, 'business_name', '') if 'business_name' in sub.columns else ''\n",
        "                cat = getattr(r, 'category', '') if 'category' in sub.columns else ''\n",
        "                rows.append(label_with_guard(t, plc, cat))\n",
        "\n",
        "            # Add the labeling results as new columns to the chunk DataFrame\n",
        "            for k in rows[0].keys():\n",
        "                sub[k] = [row[k] for row in rows]\n",
        "\n",
        "            # Save the labeled chunk as a Parquet file\n",
        "            sub.to_parquet(part_path, index=False)\n",
        "            labeled_parts.append(sub) # Add labeled chunk to the list\n",
        "            del sub, rows # Clean up memory\n",
        "            if torch.cuda.is_available(): torch.cuda.empty_cache() # Clear GPU cache\n",
        "\n",
        "\n",
        "        # Consolidate fresh parts after labeling is complete\n",
        "        labeled_1k = pd.concat(labeled_parts, ignore_index=True)\n",
        "        labeled_1k.to_parquet(PREVIEW_PARQUET, index=False)\n",
        "        labeled_1k.to_csv(PREVIEW_CSV, index=False)\n",
        "        print(\"💾 Saved preview from fresh labeling:\", PREVIEW_PARQUET, \"|\", PREVIEW_CSV)\n",
        "\n",
        "# Quick stats on the labeled data (if available)\n",
        "if 'labeled_1k' in globals():\n",
        "    stats = {\n",
        "        \"rows\": len(labeled_1k),\n",
        "        \"ads\": int(labeled_1k.get('is_advertisement', pd.Series(dtype=int)).sum()) if 'is_advertisement' in labeled_1k else None,\n",
        "        \"irrel\": int(labeled_1k.get('is_irrelevant', pd.Series(dtype=int)).sum()) if 'is_irrelevant' in labeled_1k else None,\n",
        "        \"non_visit\": int(labeled_1k.get('is_rant_without_visit', pd.Series(dtype=int)).sum()) if 'is_rant_without_visit' in labeled_1k else None,\n",
        "        \"spam\": int(labeled_1k.get('is_spam', pd.Series(dtype=int)).sum()) if 'is_spam' in labeled_1k else None,\n",
        "        \"relevant\": int(labeled_1k.get('is_relevant_overall', pd.Series(dtype=int)).sum()) if 'is_relevant_overall' in labeled_1k else None, # Added relevant count\n",
        "    }\n",
        "    print(\"\\nLabeled Data Statistics:\")\n",
        "    for key, value in stats.items():\n",
        "        if value is not None:\n",
        "            print(f\"- {key}: {value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58e30104"
      },
      "source": [
        "# Cell 9 — Build ML‑ready split (preview‑only, multilabel stratified)\n",
        "\n",
        "This cell prepares the labeled preview data for machine learning by:\n",
        "- Loading the labeled data.\n",
        "- Creating binary target columns for each violation type and overall relevance.\n",
        "- Performing a multilabel stratified shuffle split to create training, validation, and test sets, ensuring that the distribution of labels is maintained across the splits.\n",
        "- Saving the split data to a Parquet file.\n",
        "- Printing the support (number of positive instances) for each label in each split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fc8dc01"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "\n",
        "# Ensure the labeled preview file exists\n",
        "assert os.path.exists(PREVIEW_PARQUET), f\"Labeled preview file not found at {PREVIEW_PARQUET}. Run Cell 8.\"\n",
        "\n",
        "# Load the labeled data and drop duplicates based on text\n",
        "base = pd.read_parquet(PREVIEW_PARQUET).drop_duplicates(subset=['text']).reset_index(drop=True)\n",
        "base['text'] = base['text'].fillna('') # Fill any potential missing text values\n",
        "\n",
        "# Create binary target columns for each label\n",
        "base['y_relevant']       = base['is_relevant_overall'].astype(int)\n",
        "base['y_ads']            = base['is_advertisement'].astype(int)\n",
        "base['y_irrelevant']     = base['is_irrelevant'].astype(int)\n",
        "base['y_non_visit_rant'] = base['is_rant_without_visit'].astype(int)\n",
        "base['y_spam']           = base['is_spam'].astype(int)\n",
        "\n",
        "# Define the target variable array for stratification\n",
        "Y = base[['y_ads','y_irrelevant','y_non_visit_rant','y_spam','y_relevant']].values.astype(int)\n",
        "X_dummy = np.zeros((len(base), 1)) # A dummy feature array is needed for split\n",
        "\n",
        "# Perform the first split (train vs temp for val/test)\n",
        "msss1 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=42)\n",
        "train_idx, tmp_idx = next(msss1.split(X_dummy, Y))\n",
        "\n",
        "# Perform the second split (val vs test) on the temporary set\n",
        "msss2 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=43) # Different random state\n",
        "val_rel, test_rel = next(msss2.split(np.zeros((len(tmp_idx),1)), Y[tmp_idx])) # Split on the temporary indices\n",
        "\n",
        "# Map temporary indices back to original indices\n",
        "val_idx  = tmp_idx[val_rel]\n",
        "test_idx = tmp_idx[test_rel]\n",
        "\n",
        "# Create a 'split' column in the DataFrame\n",
        "split = np.array(['train']*len(base))\n",
        "split[val_idx]  = 'val'\n",
        "split[test_idx] = 'test'\n",
        "base['split'] = split\n",
        "\n",
        "# Define the output path for the ML-ready data\n",
        "ML_READY_PARQUET = f\"{OUT_DIR}/labeled_reviews_ml_ready.parquet\"\n",
        "base.to_parquet(ML_READY_PARQUET, index=False) # Save the ML-ready DataFrame\n",
        "print(\"Saved ML‑ready →\", ML_READY_PARQUET)\n",
        "\n",
        "# Print supports for each label in each split to verify stratification\n",
        "sup = lambda d: d[['y_ads','y_irrelevant','y_non_visit_rant','y_spam','y_relevant']].sum().to_dict()\n",
        "print(\"\\nLabel Supports by Split:\")\n",
        "print(\"supports train:\", sup(base[base.split=='train']))\n",
        "print(\"supports val:  \", sup(base[base.split=='val']))\n",
        "print(\"supports test: \", sup(base[base.split=='test']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "576b2ad3"
      },
      "source": [
        "# Cell 10 — Train baseline (TF‑IDF + Logistic Regression)\n",
        "\n",
        "This cell trains a baseline machine learning model for each defined label using a combination of TF-IDF word and character features and Logistic Regression. It also finds the optimal classification threshold for each label based on the F1 score on the validation set and saves the trained models, vectorizers, thresholds, and performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31cd880a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support, precision_recall_curve\n",
        "from scipy.sparse import hstack\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Ensure the ML-ready data and necessary directories exist\n",
        "assert os.path.exists(ML_READY_PARQUET), f\"ML-ready data not found at {ML_READY_PARQUET}. Run Cell 9.\"\n",
        "\n",
        "# Load the ML-ready data and split into train, validation, and test sets\n",
        "ml = pd.read_parquet(ML_READY_PARQUET)\n",
        "train = ml[ml['split']=='train'].copy()\n",
        "val   = ml[ml['split']=='val'].copy()\n",
        "test  = ml[ml['split']=='test'].copy()\n",
        "\n",
        "# Fill missing text values with empty strings\n",
        "train['text'] = train['text'].fillna('')\n",
        "val['text']   = val['text'].fillna('')\n",
        "test['text']  = test['text'].fillna('')\n",
        "\n",
        "# Initialize TF-IDF vectorizers for words and characters\n",
        "word_vec = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.9, max_features=200_000)\n",
        "char_vec = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=2, max_features=300_000)\n",
        "\n",
        "# Fit and transform the training data, and transform validation and test data\n",
        "Xw_tr = word_vec.fit_transform(train['text']); Xc_tr = char_vec.fit_transform(train['text']); X_tr = hstack([Xw_tr, Xc_tr]).tocsr()\n",
        "Xw_v  = word_vec.transform(val['text']);   Xc_v  = char_vec.transform(val['text']);   X_v  = hstack([Xw_v,  Xc_v ]).tocsr()\n",
        "Xw_te = word_vec.transform(test['text']);  Xc_te = char_vec.transform(test['text']);  X_te = hstack([Xw_te, Xc_te]).tocsr()\n",
        "\n",
        "# Define the binary labels and their corresponding column names\n",
        "LABELS_BIN = [\n",
        "    ('y_relevant', 'relevant'),\n",
        "    ('y_ads', 'ads'),\n",
        "    ('y_irrelevant', 'irrelevant'),\n",
        "    ('y_non_visit_rant', 'non_visit'),\n",
        "    ('y_spam', 'spam'),\n",
        "]\n",
        "\n",
        "models = {} # Dictionary to store trained models\n",
        "thresholds = {} # Dictionary to store optimal thresholds\n",
        "metrics_rows = [] # List to store performance metrics\n",
        "\n",
        "# Train a Logistic Regression model for each label\n",
        "for col, name in LABELS_BIN:\n",
        "    y_tr = train[col].astype(int).values\n",
        "    y_v  = val[col].astype(int).values\n",
        "    y_te = test[col].astype(int).values\n",
        "\n",
        "    # Initialize and train the Logistic Regression classifier with balanced class weights\n",
        "    clf = LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear')\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    models[name] = clf\n",
        "\n",
        "    # Predict probabilities on the validation set\n",
        "    p_v = clf.predict_proba(X_v)[:,1]\n",
        "\n",
        "    # Find the optimal threshold on the validation set by maximizing F1 score\n",
        "    grid = np.linspace(0.1, 0.9, 17) # Search grid for thresholds\n",
        "    f1s = [f1_score(y_v, (p_v>=t).astype(int), zero_division=0) for t in grid]\n",
        "    t_best = float(grid[int(np.argmax(f1s))]) # Threshold yielding the highest F1\n",
        "    thresholds[name] = t_best\n",
        "\n",
        "    # Evaluate the model on validation and test sets using the best threshold\n",
        "    for split_name, y_true, X in [('val', y_v, X_v), ('test', y_te, X_te)]:\n",
        "        p = clf.predict_proba(X)[:,1]\n",
        "        y_hat = (p >= t_best).astype(int)\n",
        "        # Calculate precision, recall, and F1 score\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_hat, average='binary', zero_division=0)\n",
        "        metrics_rows.append({'label': name, 'split': split_name, 'support_pos': int(y_true.sum()),\n",
        "                             'precision': round(float(prec),4), 'recall': round(float(rec),4),\n",
        "                             'f1': round(float(f1),4), 'threshold': t_best})\n",
        "\n",
        "# Create a DataFrame from the performance metrics\n",
        "metrics = pd.DataFrame(metrics_rows)\n",
        "print(\"F1 Scores by Label and Split:\")\n",
        "# Display F1 scores in a pivot table format\n",
        "print(metrics.pivot(index='label', columns='split', values='f1').fillna(0))\n",
        "\n",
        "# Define the directory to save models and results\n",
        "MODEL_DIR = f\"{OUT_DIR}/models/baseline_tfidf_lr\"; os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Save the trained vectorizers, models, thresholds, and metrics\n",
        "joblib.dump(word_vec, f\"{MODEL_DIR}/tfidf_word.joblib\")\n",
        "joblib.dump(char_vec, f\"{MODEL_DIR}/tfidf_char.joblib\")\n",
        "for name, clf in models.items(): joblib.dump(clf, f\"{MODEL_DIR}/clf_{name}.joblib\")\n",
        "with open(f\"{MODEL_DIR}/thresholds.json\", 'w') as f: json.dump(thresholds, f, indent=2)\n",
        "metrics.to_csv(f\"{MODEL_DIR}/metrics.csv\", index=False)\n",
        "print(f\"\\nSaved models and metrics to: {MODEL_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ebad9e"
      },
      "source": [
        "# Cell 11 — Error analysis (val/test), save misclassifications\n",
        "\n",
        "This cell performs error analysis on the validation and test sets using the trained baseline models. It calculates performance metrics again and saves misclassified instances (False Positives and False Negatives) to CSV files for further inspection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c56f1f0"
      },
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Ensure ML-ready data and model directory exist\n",
        "assert os.path.exists(ML_READY_PARQUET), f\"ML-ready data not found at {ML_READY_PARQUET}. Run Cell 9.\"\n",
        "assert os.path.exists(MODEL_DIR), f\"Model directory not found at {MODEL_DIR}. Run Cell 10.\"\n",
        "\n",
        "\n",
        "# Load the ML-ready data and split into validation and test sets\n",
        "ml = pd.read_parquet(ML_READY_PARQUET)\n",
        "val   = ml[ml['split']=='val'].copy()\n",
        "test  = ml[ml['split']=='test'].copy()\n",
        "\n",
        "# Load the saved vectorizers, models, and thresholds\n",
        "word_vec = joblib.load(f\"{MODEL_DIR}/tfidf_word.joblib\")\n",
        "char_vec = joblib.load(f\"{MODEL_DIR}/tfidf_char.joblib\")\n",
        "models   = {lbl: joblib.load(f\"{MODEL_DIR}/clf_{lbl}.joblib\") for lbl in ['relevant','ads','irrelevant','non_visit','spam']}\n",
        "with open(f\"{MODEL_DIR}/thresholds.json\") as f: thresholds = json.load(f)\n",
        "\n",
        "# Define the labels and their corresponding target column names\n",
        "LABELS = [\n",
        "    ('relevant','y_relevant'),\n",
        "    ('ads','y_ads'),\n",
        "    ('irrelevant','y_irrelevant'),\n",
        "    ('non_visit','y_non_visit_rant'),\n",
        "    ('spam','y_spam'),\n",
        "]\n",
        "\n",
        "def make_X(df_: pd.DataFrame):\n",
        "    \"\"\"Applies the trained TF-IDF vectorizers to a DataFrame's text column.\"\"\"\n",
        "    Xw = word_vec.transform(df_['text'].fillna(''))\n",
        "    Xc = char_vec.transform(df_['text'].fillna(''))\n",
        "    return hstack([Xw, Xc]).tocsr()\n",
        "\n",
        "# Create feature matrices for validation and test sets\n",
        "Xv = make_X(val)\n",
        "Xt = make_X(test)\n",
        "\n",
        "def eval_split(name: str, df: pd.DataFrame, X):\n",
        "    \"\"\"Evaluates the model on a given split and saves misclassified instances.\"\"\"\n",
        "    rows = []\n",
        "    for lbl, ycol in LABELS:\n",
        "        y_true = df[ycol].astype(int).values\n",
        "        p = models[lbl].predict_proba(X)[:,1]\n",
        "        th = thresholds[lbl]\n",
        "        y_hat = (p >= th).astype(int)\n",
        "        # Calculate performance metrics for the current label and split\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_hat, average='binary', zero_division=0)\n",
        "        rows.append({'label': lbl, 'split': name, 'precision':prec, 'recall':rec, 'f1':f1,\n",
        "                     'support_pos': int(y_true.sum()), 'threshold': th})\n",
        "\n",
        "        # Save False Positives (True=0, Predicted=1)\n",
        "        df_fp = df[['text']].copy()\n",
        "        df_fp['y_true'] = y_true\n",
        "        df_fp['p'] = p\n",
        "        df_fp['y_hat'] = y_hat\n",
        "        df_fp[(df_fp.y_true==0)&(df_fp.y_hat==1)].sort_values('p', ascending=False).head(200).to_csv(f\"{MODEL_DIR}/misclf_{lbl}_{name}_FP.csv\", index=False)\n",
        "\n",
        "        # Save False Negatives (True=1, Predicted=0)\n",
        "        df_fn = df[['text']].copy()\n",
        "        df_fn['y_true'] = y_true\n",
        "        df_fn['p'] = p\n",
        "        df_fn['y_hat'] = y_hat\n",
        "        df_fn[(df_fn.y_true==1)&(df_fn.y_hat==0)].sort_values('p', ascending=True ).head(200).to_csv(f\"{MODEL_DIR}/misclf_{lbl}_{name}_FN.csv\", index=False)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Evaluate on validation and test sets\n",
        "m_val  = eval_split('val',  val,  Xv)\n",
        "m_test = eval_split('test', test, Xt)\n",
        "\n",
        "# Concatenate metrics and display F1 scores\n",
        "metrics2 = pd.concat([m_val, m_test], ignore_index=True)\n",
        "print(\"F1 Scores from Error Analysis:\")\n",
        "print(metrics2.pivot(index='label', columns='split', values='f1').round(4))\n",
        "print(f\"\\nSaved misclassification CSVs to: {MODEL_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45548d34"
      },
      "source": [
        "# Cell 12 — Threshold calibration from PR curves (val)\n",
        "\n",
        "This optional cell recalibrates the classification thresholds for each label using the Precision-Recall curve on the validation set. By default, it aims to maximize the F1 score, but you can customize the calibration target. The updated thresholds are saved to `thresholds.json`, overwriting the previous ones. After running this cell, you should re-run Cell 11 to evaluate performance with the new thresholds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bbc4bd0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import joblib\n",
        "\n",
        "# Ensure necessary files and variables exist\n",
        "assert os.path.exists(ML_READY_PARQUET), f\"ML-ready data not found at {ML_READY_PARQUET}. Run Cell 9.\"\n",
        "assert os.path.exists(MODEL_DIR), f\"Model directory not found at {MODEL_DIR}. Run Cell 10.\"\n",
        "\n",
        "# Load validation data\n",
        "ml = pd.read_parquet(ML_READY_PARQUET)\n",
        "val   = ml[ml['split']=='val'].copy()\n",
        "\n",
        "# Load models and current thresholds\n",
        "models   = {lbl: joblib.load(f\"{MODEL_DIR}/clf_{lbl}.joblib\") for lbl in ['relevant','ads','irrelevant','non_visit','spam']}\n",
        "with open(f\"{MODEL_DIR}/thresholds.json\") as f: thresholds = json.load(f)\n",
        "\n",
        "# Recalibration target (default is 'f1')\n",
        "CALIB_TARGET = 'f1'\n",
        "\n",
        "new_thresholds = {} # Dictionary to store the newly calibrated thresholds\n",
        "rows = [] # List to store calibration results\n",
        "\n",
        "# Create feature matrix for validation data\n",
        "Xv = make_X(val) # make_X is defined in Cell 11\n",
        "\n",
        "# Iterate through each label to calibrate the threshold\n",
        "for lbl, ycol in LABELS: # LABELS is defined in Cell 11\n",
        "    y_true = val[ycol].astype(int).values\n",
        "    p = models[lbl].predict_proba(Xv)[:,1] # Get predicted probabilities\n",
        "\n",
        "    # Calculate Precision-Recall curve\n",
        "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
        "\n",
        "    # Calculate F1 scores for each threshold\n",
        "    f1 = (2*prec*rec)/(prec+rec + 1e-9) # Add a small epsilon to avoid division by zero\n",
        "\n",
        "    # Find the threshold that maximizes the F1 score\n",
        "    i = int(np.nanargmax(f1))\n",
        "    t_best = float(thr[max(0, min(i, len(thr)-1))]) # Get the corresponding threshold, handle edge cases\n",
        "\n",
        "    # Store the old and new thresholds\n",
        "    rows.append({'label': lbl, 'old': thresholds[lbl], 'new': t_best})\n",
        "    new_thresholds[lbl] = t_best\n",
        "\n",
        "# Display the calibration results\n",
        "calib = pd.DataFrame(rows)\n",
        "print(\"Threshold Calibration Results:\")\n",
        "print(calib)\n",
        "\n",
        "# Save the new thresholds, overwriting the old ones\n",
        "with open(f\"{MODEL_DIR}/thresholds.json\", 'w') as f: json.dump(new_thresholds, f, indent=2)\n",
        "print(f\"\\nUpdated thresholds.json in: {MODEL_DIR}\")\n",
        "\n",
        "# Note: After running this cell, you should re-run Cell 11 to evaluate performance with the new thresholds."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05476bfe"
      },
      "source": [
        "# Cell 13 — Batch inference utility (score any DataFrame)\n",
        "\n",
        "This cell provides a utility function `predict_df` to apply the trained baseline models to a new DataFrame for batch inference. It takes a DataFrame and the name of the text column as input, applies the TF-IDF vectorizers, uses the trained Logistic Regression models with the calibrated thresholds to predict labels, and returns the original DataFrame with added columns for predicted probabilities and binary labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1573eaee"
      },
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "from scipy.sparse import hstack\n",
        "from tqdm.auto import tqdm # Import tqdm\n",
        "\n",
        "# Ensure model directory exists and load necessary components\n",
        "assert os.path.exists(MODEL_DIR), f\"Model directory not found at {MODEL_DIR}. Run Cell 10.\"\n",
        "\n",
        "# Load the saved vectorizers, models, and thresholds\n",
        "word_vec = joblib.load(f\"{MODEL_DIR}/tfidf_word.joblib\")\n",
        "char_vec = joblib.load(f\"{MODEL_DIR}/tfidf_char.joblib\")\n",
        "models   = {lbl: joblib.load(f\"{MODEL_DIR}/clf_{lbl}.joblib\") for lbl in ['relevant','ads','irrelevant','non_visit','spam']}\n",
        "with open(f\"{MODEL_DIR}/thresholds.json\") as f: thresholds = json.load(f) # Reload thresholds (useful after calibration in Cell 12)\n",
        "\n",
        "# Define the labels (needed for predicting specific columns)\n",
        "LABELS = [\n",
        "    ('relevant','y_relevant'),\n",
        "    ('ads','y_ads'),\n",
        "    ('irrelevant','y_irrelevant'),\n",
        "    ('non_visit','y_non_visit_rant'),\n",
        "    ('spam','y_spam'),\n",
        "]\n",
        "\n",
        "# make_X function is defined in Cell 11, ensure it has been run or define it here if running this cell standalone\n",
        "def make_X(df_: pd.DataFrame):\n",
        "    \"\"\"Applies the trained TF-IDF vectorizers to a DataFrame's text column.\"\"\"\n",
        "    Xw = word_vec.transform(df_['text'].fillna(''))\n",
        "    Xc = char_vec.transform(df_['text'].fillna(''))\n",
        "    return hstack([Xw, Xc]).tocsr()\n",
        "\n",
        "\n",
        "def predict_df(df_new: pd.DataFrame, text_col='text', batch=20000):\n",
        "    \"\"\"\n",
        "    Applies the trained baseline models to a new DataFrame for batch inference.\n",
        "\n",
        "    Args:\n",
        "        df_new (pd.DataFrame): The input DataFrame to score.\n",
        "        text_col (str): The name of the column containing the text to be used for prediction.\n",
        "        batch (int): The batch size for processing the DataFrame to manage memory.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The original DataFrame with added columns for predicted\n",
        "                      probabilities (p_<label>) and binary predictions (<label>).\n",
        "    \"\"\"\n",
        "    assert text_col in df_new.columns, f\"Text column '{text_col}' not found in the input DataFrame.\"\n",
        "\n",
        "    N = len(df_new) # Total number of rows in the input DataFrame\n",
        "    out = [] # List to store results from each batch\n",
        "\n",
        "    # Process the DataFrame in batches with a progress bar\n",
        "    for start in tqdm(range(0, N, batch), desc=\"Scoring batches\"): # Added tqdm here\n",
        "        sl = df_new.iloc[start:start+batch].copy() # Get the current batch slice\n",
        "        # Create feature matrix for the current batch\n",
        "        X  = make_X(sl)\n",
        "\n",
        "        block = {'idx': sl.index} # Store the original index\n",
        "        # Predict probabilities and binary labels for each label\n",
        "        for lbl, _ in LABELS:\n",
        "            p = models[lbl].predict_proba(X)[:,1] # Get probability of the positive class\n",
        "            block[f'p_{lbl}'] = p # Store predicted probabilities\n",
        "            block[f'{lbl}']   = (p >= thresholds[lbl]).astype(int) # Apply threshold for binary prediction\n",
        "        out.append(pd.DataFrame(block)) # Add batch results to the list\n",
        "\n",
        "    # Concatenate batch results, set index to original index, and merge with original DataFrame\n",
        "    pred = pd.concat(out, ignore_index=True).set_index('idx').reindex(df_new.index)\n",
        "    return pd.concat([df_new, pred], axis=1) # Return original DataFrame with predictions\n",
        "\n",
        "\n",
        "# Example usage (uncomment to run a small example)\n",
        "# print(\"Running example prediction on the first 100 rows of ml data...\")\n",
        "# try:\n",
        "#     # Ensure ml data is available (from Cell 9)\n",
        "#     if 'ml' not in globals():\n",
        "#          assert os.path.exists(ML_READY_PARQUET), f\"ML-ready data not found at {ML_READY_PARQUET}. Run Cell 9.\"\n",
        "#          ml = pd.read_parquet(ML_READY_PARQUET)\n",
        "#\n",
        "#     scored = predict_df(ml.head(100))\n",
        "#     display(scored.head())\n",
        "#     print(\"\\nExample prediction complete.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"\\nError during example prediction: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the trained model to the entire df DataFrame\n",
        "print(\"Running prediction on the entire df DataFrame...\")\n",
        "\n",
        "# Ensure df data is available (from Cell 3)\n",
        "if 'df' not in globals():\n",
        "     assert os.path.exists(REVIEW_PATH) and os.path.exists(META_PATH), \"Review/meta files not found. Run Cell 3.\"\n",
        "     # Reload df if not in globals (assuming Cell 3's logic)\n",
        "     df_reviews = parse_jsonl(REVIEW_PATH)\n",
        "     df_meta    = parse_jsonl(META_PATH)\n",
        "     if 'gmap_id' in df_meta.columns:\n",
        "         df_meta = df_meta.drop_duplicates(subset=['gmap_id'])\n",
        "     df = df_reviews.merge(df_meta, on='gmap_id', how='left')\n",
        "     keep_cols = [c for c in ['user_id','gmap_id','text','time','rating'] if c in df.columns]\n",
        "     if keep_cols:\n",
        "         df = df.drop_duplicates(subset=keep_cols, keep='first')\n",
        "     ren = {}\n",
        "     if 'name_x' in df.columns: ren['name_x'] = 'reviewer_name'\n",
        "     if 'name_y' in df.columns: ren['name_y'] = 'business_name'\n",
        "     if ren:\n",
        "         df = df.rename(columns=ren)\n",
        "\n",
        "\n",
        "try:\n",
        "    scored_df = predict_df(df)\n",
        "    print(\"\\nPrediction complete. Displaying head of scored DataFrame:\")\n",
        "    display(scored_df.head())\n",
        "    print(\"\\nScored DataFrame shape:\", scored_df.shape)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during prediction on entire df: {e}\")"
      ],
      "metadata": {
        "id": "PYcg1uxZ_9hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the distribution of the predicted labels\n",
        "print(scored_df['relevant'].value_counts())\n",
        "print(scored_df['ads'].value_counts())\n",
        "print(scored_df['irrelevant'].value_counts())\n",
        "print(scored_df['non_visit'].value_counts())\n",
        "print(scored_df['spam'].value_counts())"
      ],
      "metadata": {
        "id": "jezJ3GIsKvmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fca2890f"
      },
      "source": [
        "### Examples of Reviews Predicted as 'ads'\n",
        "\n",
        "This cell displays the reviewer name, rating, full text, and business name for the first 10 reviews that were predicted to contain advertisements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a9b6733"
      },
      "source": [
        "# Display examples of reviews predicted as 'ads'\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None) # Ensure full text is displayed\n",
        "\n",
        "if 'scored_df' in globals():\n",
        "    display(scored_df[scored_df['ads']==1][['reviewer_name','rating','text','business_name']].head(10))\n",
        "else:\n",
        "    print(\"The 'scored_df' DataFrame is not available. Please run the prediction cells first.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fdc5abf"
      },
      "source": [
        "### Examples of Reviews Predicted as 'irrelevant'\n",
        "\n",
        "This cell displays the reviewer name, rating, full text, and business name for the first 10 reviews that were predicted to be irrelevant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2c1d64b"
      },
      "source": [
        "# Display examples of reviews predicted as 'irrelevant'\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None) # Ensure full text is displayed\n",
        "\n",
        "if 'scored_df' in globals():\n",
        "    display(scored_df[scored_df['irrelevant']==1][['reviewer_name','rating','text','business_name']].head(10))\n",
        "else:\n",
        "    print(\"The 'scored_df' DataFrame is not available. Please run the prediction cells first.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f519689e"
      },
      "source": [
        "### Examples of Reviews Predicted as 'non_visit_rant'\n",
        "\n",
        "This cell displays the reviewer name, rating, full text, and business name for the first 10 reviews that were predicted to be rants without a clear visit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d5fc77e"
      },
      "source": [
        "# Display examples of reviews predicted as 'non_visit_rant'\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None) # Ensure full text is displayed\n",
        "\n",
        "if 'scored_df' in globals():\n",
        "    display(scored_df[scored_df['non_visit']==1][['reviewer_name','rating','text','business_name']].head(10))\n",
        "else:\n",
        "    print(\"The 'scored_df' DataFrame is not available. Please run the prediction cells first.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6945a14"
      },
      "source": [
        "### Examples of Reviews Predicted as 'spam'\n",
        "\n",
        "This cell displays the reviewer name, rating, full text, and business name for the first 10 reviews that were predicted to be spam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b1fbe2d"
      },
      "source": [
        "# Display examples of reviews predicted as 'spam'\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None) # Ensure full text is displayed\n",
        "\n",
        "if 'scored_df' in globals():\n",
        "    display(scored_df[scored_df['spam']==1][['reviewer_name','rating','text','business_name']].head(10))\n",
        "else:\n",
        "    print(\"The 'scored_df' DataFrame is not available. Please run the prediction cells first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}